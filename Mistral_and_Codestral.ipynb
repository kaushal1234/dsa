{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaushal1234/dsa/blob/main/Mistral_and_Codestral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjxmC4FoudUq",
        "outputId": "c5d82faa-672f-4aa7-9595-c407f5339510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: langchain-mistralai in /usr/local/lib/python3.10/dist-packages (0.1.10)\n",
            "Requirement already satisfied: llama_index in /usr/local/lib/python3.10/dist-packages (0.10.55)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.10/dist-packages (0.1.8)\n",
            "Requirement already satisfied: MistralAI in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.19)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.86)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from langchain-mistralai) (0.27.0)\n",
            "Requirement already satisfied: httpx-sse<1,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from langchain-mistralai) (0.4.0)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from langchain-mistralai) (0.19.1)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.2.8)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.12)\n",
            "Requirement already satisfied: llama-index-core==0.10.55 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.10.55)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.10)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.2.5)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.25)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.7)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.30)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.6)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.55->llama_index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.55->llama_index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.55->llama_index) (2023.6.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.55->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.55->llama_index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.55->llama_index) (3.8.1)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.55->llama_index) (1.35.14)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.55->llama_index) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.55->llama_index) (9.4.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.55->llama_index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.55->llama_index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.55->llama_index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.55->llama_index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.55->llama_index) (1.14.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: orjson<3.11,>=3.9.10 in /usr/local/lib/python3.10/dist-packages (from MistralAI) (3.10.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain-mistralai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain_community) (24.1)\n",
            "Requirement already satisfied: llama-cloud>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from llama-index-indices-managed-llama-cloud>=0.2.0->llama_index) (0.0.9)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.3.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse>=0.1.2->llama_index) (0.4.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15.1->langchain-mistralai) (0.23.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (3.15.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.55->llama_index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.55->llama_index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.55->llama_index) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core==0.10.55->llama_index) (1.7.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.25.2->langchain-mistralai) (1.2.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.55->llama_index) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.55->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.55->llama_index) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.55->llama_index) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.55->llama_index) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "! pip install -U langchain_community langchain-mistralai llama_index langchain langgraph MistralAI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup LLM"
      ],
      "metadata": {
        "id": "Aes4dFwq2SY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-mistralai\n",
        "import os\n",
        "\n",
        "os.environ[\"MISTRAL_API_KEY\"] = \"FXTDN4Aka3kd8IM6s87ZXu1XZ0QjvTej\"\n",
        "\n",
        "from llama_index.llms.mistralai import MistralAI\n",
        "\n",
        "llm = MistralAI(model=\"codestral-latest\", temperature=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqoh_olaujMG",
        "outputId": "66644de0-f7f1-4463-d7d7-7580dc8ab695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-llms-mistralai in /usr/local/lib/python3.10/dist-packages (0.1.17)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.39 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-mistralai) (0.10.55)\n",
            "Requirement already satisfied: mistralai>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-mistralai) (0.4.2)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.35.14)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.14.1)\n",
            "Requirement already satisfied: orjson<3.11,>=3.9.10 in /usr/local/lib/python3.10/dist-packages (from mistralai>=0.4.0->llama-index-llms-mistralai) (3.10.6)\n",
            "Requirement already satisfied: pydantic<3,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from mistralai>=0.4.0->llama-index-llms-mistralai) (2.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (4.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.5.2->mistralai>=0.4.0->llama-index-llms-mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.5.2->mistralai>=0.4.0->llama-index-llms-mistralai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.39->llama-index-llms-mistralai) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instruct mode usage"
      ],
      "metadata": {
        "id": "ekOwDaZH2Vrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [ChatMessage(role=\"user\", content=\"Write a function for azure to fetch me the infromation about the azure services that we are using\")]\n",
        "\n",
        "response = llm.chat(messages)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeOZCITcu1hq",
        "outputId": "2ce3c008-893c-4104-80a7-e19b80d7ab96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Sure, I can help you with that. However, I'm an AI language model and I don't have the ability to directly interact with Azure services. I can, however, guide you on how to write a Python function using the Azure SDK to fetch information about the Azure services that you're using.\n",
            "\n",
            "Here's a basic example of how you might do this using the `azure-mgmt-resource` package:\n",
            "\n",
            "```python\n",
            "from azure.identity import DefaultAzureCredential\n",
            "from azure.mgmt.resource import ResourceManagementClient\n",
            "\n",
            "def get_azure_services():\n",
            "    # Create a credential object\n",
            "    credential = DefaultAzureCredential()\n",
            "\n",
            "    # Create a resource management client\n",
            "    subscription_id = \"<your-subscription-id>\"\n",
            "    resource_client = ResourceManagementClient(credential, subscription_id)\n",
            "\n",
            "    # Get a list of all resources in the subscription\n",
            "    resources = resource_client.resources.list()\n",
            "\n",
            "    # Filter the resources to only include Azure services\n",
            "    azure_services = [resource for resource in resources if resource.type.startswith('Microsoft.')]\n",
            "\n",
            "    return azure_services\n",
            "```\n",
            "\n",
            "This function will return a list of all Azure services that are being used in the specified subscription. Please replace `<your-subscription-id>` with your actual Azure subscription ID.\n",
            "\n",
            "Remember to install the necessary package using pip:\n",
            "\n",
            "```bash\n",
            "pip install azure-mgmt-resource\n",
            "```\n",
            "\n",
            "Also, make sure you have the necessary permissions to access the resources in your Azure subscription. The `DefaultAzureCredential` used in the function will try to get valid credentials from the environment, managed identities, or your user account (in that order).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a RAG Pipeline using LlamaIndex\n",
        "\n",
        "The following code demonstrates how to build a Retrieval-Augmented Generation (RAG) pipeline using the LlamaIndex package. The RAG pipeline combines the power of a retriever and a generator to produce more accurate and contextually relevant responses.\n",
        "\n",
        "### Importing Required Libraries\n",
        "\n",
        "First, we need to import the necessary libraries and modules:\n",
        "```python\n",
        "from llama_index import (\n",
        "    GPTSimpleVectorIndex,\n",
        "    LLMPredictor,\n",
        "    ServiceContext,\n",
        "    PromptHelper,\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI\n"
      ],
      "metadata": {
        "id": "q91aIcz12MqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"user\",\n",
        "        content=\"Write a function to build RAG pipeline using LlamaIndex.\",\n",
        "    )\n",
        "]\n",
        "\n",
        "response = llm.chat(messages)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8yz0W09vel9",
        "outputId": "8e7ad1aa-1493-4a0f-d858-aea32ce9c0b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Sure, I can help you with that. Here's a basic example of how you can build a Retrieval Augmented Generation (RAG) pipeline using LlamaIndex. This example assumes that you have a collection of documents and a query.\n",
            "\n",
            "```python\n",
            "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
            "\n",
            "# Load documents\n",
            "documents = SimpleDirectoryReader('path_to_your_documents').load_data()\n",
            "\n",
            "# Create index\n",
            "index = VectorStoreIndex.from_documents(documents)\n",
            "\n",
            "# Query the index\n",
            "query_engine = index.as_query_engine()\n",
            "response = query_engine.query(\"Your query here\")\n",
            "\n",
            "print(response)\n",
            "```\n",
            "\n",
            "In this code:\n",
            "\n",
            "1. We first import the necessary classes from LlamaIndex.\n",
            "2. We load the documents from a directory using `SimpleDirectoryReader`.\n",
            "3. We create an index from the documents using `VectorStoreIndex.from_documents`.\n",
            "4. We create a query engine from the index using `index.as_query_engine`.\n",
            "5. We then query the engine with our question.\n",
            "6. Finally, we print the response.\n",
            "\n",
            "Please replace `'path_to_your_documents'` with the path to your documents and `\"Your query here\"` with your actual query.\n",
            "\n",
            "This is a basic example. Depending on your specific use case, you might need to use a different type of index, a different type of reader, or add additional steps to the pipeline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fill-in-the-middle\n",
        "\n",
        "This feature allows users to set a starting point with a prompt and an optional ending with a suffix and stop. The Codestral model then generates the intervening code, perfect for tasks requiring specific code generation.\n"
      ],
      "metadata": {
        "id": "kqaX7kP119j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"def multiply(\"\n",
        "suffix = \"return a*b\"\n",
        "\n",
        "response = llm.fill_in_middle(prompt, suffix)\n",
        "\n",
        "print(\n",
        "    f\"\"\"\n",
        "{prompt}\n",
        "{response.text}\n",
        "{suffix}\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG5bU6G6126f",
        "outputId": "c330cc7d-4219-44ed-bdcc-56cb2f197830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "def multiply(\n",
            "a,b):\n",
            "\n",
            "    \"\"\"\n",
            "    This function multiplies two numbers\n",
            "\n",
            "    Arguments:\n",
            "    a: first number to multiply\n",
            "    b: second number to multiply\n",
            "\n",
            "    Returns:\n",
            "    the product of a and b\n",
            "    \"\"\"\n",
            "\n",
            "    \n",
            "return a*b\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fill the code with start, end of the code and stop tokens."
      ],
      "metadata": {
        "id": "FfTfv4Ec2d9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"def multiply(a,\"\n",
        "suffix = \"\"\n",
        "stop = [\"\\n\\n\\n\"]\n",
        "\n",
        "response = llm.fill_in_middle(prompt, suffix, stop)\n",
        "\n",
        "print(\n",
        "    f\"\"\"\n",
        "{prompt}\n",
        "{response.text}\n",
        "{suffix}\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8AOmTN815vT",
        "outputId": "d7a78281-5e82-422c-8f81-1db978ec6358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "def multiply(a,\n",
            " b):\n",
            "\n",
            "    # Initialize result\n",
            "    result = 0\n",
            "\n",
            "    # Multiply a with b\n",
            "    # using bitwise operators\n",
            "    while (b > 0):\n",
            "\n",
            "        # If b is odd, add a to result\n",
            "        if (b & 1):\n",
            "            result = result + a\n",
            "\n",
            "        # Double the value of a\n",
            "        a = a << 1\n",
            "\n",
            "        # Halve the value of b\n",
            "        b = b >> 1\n",
            "\n",
            "    return result\n",
            "\n",
            "# Driver code\n",
            "a = 12\n",
            "b = 10\n",
            "print(multiply(a, b))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistralai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iol57s6u8loi",
        "outputId": "37addfdb-fc57-4e22-ee96-9a65fdd591fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mistralai in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: httpx<1,>=0.25 in /usr/local/lib/python3.10/dist-packages (from mistralai) (0.27.0)\n",
            "Requirement already satisfied: orjson<3.11,>=3.9.10 in /usr/local/lib/python3.10/dist-packages (from mistralai) (3.10.6)\n",
            "Requirement already satisfied: pydantic<3,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from mistralai) (2.8.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25->mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25->mistralai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25->mistralai) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25->mistralai) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25->mistralai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.25->mistralai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.5.2->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.5.2->mistralai) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.5.2->mistralai) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.25->mistralai) (1.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "\n",
        "def mistral(user_message,\n",
        "            model=\"mistral-small-latest\",\n",
        "            is_json=False):\n",
        "    client = MistralClient(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
        "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
        "\n",
        "    if is_json:\n",
        "        chat_response = client.chat(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            response_format={\"type\": \"json_object\"})\n",
        "    else:\n",
        "        chat_response = client.chat(\n",
        "            model=model,\n",
        "            messages=messages)\n",
        "\n",
        "    return chat_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "JEhEymQS9Zit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "    You are a bank customer service bot.\n",
        "    Your task is to assess customer intent and categorize customer\n",
        "    inquiry after <<<>>> into one of the following predefined categories:\n",
        "\n",
        "    card arrival\n",
        "    change pin\n",
        "    exchange rate\n",
        "    country support\n",
        "    cancel transfer\n",
        "    charge dispute\n",
        "\n",
        "    If the text doesn't fit into any of the above categories,\n",
        "    classify it as:\n",
        "    customer service\n",
        "\n",
        "    You will only respond with the predefined category.\n",
        "    Do not provide explanations or notes.\n",
        "\n",
        "    ###\n",
        "    Here are some examples:\n",
        "\n",
        "    Inquiry: How do I know if I will get my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?\n",
        "    Category: card arrival\n",
        "    Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.\n",
        "    Category: exchange rate\n",
        "    Inquiry: What countries are getting support? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.\n",
        "    Category: country support\n",
        "    Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue.\n",
        "    Category: customer service\n",
        "    ###\n",
        "\n",
        "    <<<\n",
        "    Inquiry: {inquiry}\n",
        "    >>>\n",
        "    Category:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7XPuScck6GIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOEdDukq8de1",
        "outputId": "f44eeab5-fe48-4655-a7ae-0ae32215a8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " b):\n",
            "\n",
            "    # Initialize result\n",
            "    result = 0\n",
            "\n",
            "    # Multiply a with b\n",
            "    # using bitwise operators\n",
            "    while (b > 0):\n",
            "\n",
            "        # If b is odd, add a to result\n",
            "        if (b & 1):\n",
            "            result = result + a\n",
            "\n",
            "        # Double the value of a\n",
            "        a = a << 1\n",
            "\n",
            "        # Halve the value of b\n",
            "        b = b >> 1\n",
            "\n",
            "    return result\n",
            "\n",
            "# Driver code\n",
            "a = 12\n",
            "b = 10\n",
            "print(multiply(a, b))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "\n",
        "def mistral(user_message,\n",
        "            model=\"mistral-small-latest\",\n",
        "            is_json=False):\n",
        "    client = MistralClient(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
        "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
        "\n",
        "    if is_json:\n",
        "        chat_response = client.chat(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            response_format={\"type\": \"json_object\"})\n",
        "    else:\n",
        "        chat_response = client.chat(\n",
        "            model=model,\n",
        "            messages=messages)\n",
        "\n",
        "    return chat_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "Ws5UT3Hr8dcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medical_notes = \"\"\"\n",
        "A 60-year-old male patient, Mr. Johnson, presented with symptoms\n",
        "of increased thirst, frequent urination, fatigue, and unexplained\n",
        "weight loss. Upon evaluation, he was diagnosed with diabetes,\n",
        "confirmed by elevated blood sugar levels. Mr. Johnson's weight\n",
        "is 210 lbs. He has been prescribed Metformin to be taken twice daily\n",
        "with meals. It was noted during the consultation that the patient is\n",
        "a current smoker.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DHC4VCN59EUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Extract information from the following medical notes:\n",
        "{medical_notes}\n",
        "\n",
        "Return json format with the following JSON schema:\n",
        "\n",
        "{{\n",
        "        \"age\": {{\n",
        "            \"type\": \"integer\"\n",
        "        }},\n",
        "        \"gender\": {{\n",
        "            \"type\": \"string\",\n",
        "            \"enum\": [\"male\", \"female\", \"other\"]\n",
        "        }},\n",
        "        \"diagnosis\": {{\n",
        "            \"type\": \"string\",\n",
        "            \"enum\": [\"migraine\", \"diabetes\", \"arthritis\", \"acne\"]\n",
        "        }},\n",
        "        \"weight\": {{\n",
        "            \"type\": \"integer\"\n",
        "        }},\n",
        "        \"smoking\": {{\n",
        "            \"type\": \"string\",\n",
        "            \"enum\": [\"yes\", \"no\"]\n",
        "        }}\n",
        "}}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "G1EO6n5T8dYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = mistral(prompt, is_json=True)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ywKiawb8dWb",
        "outputId": "2439455b-d78e-4800-c9af-7d7715bb07a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"age\": 60, \"gender\": \"male\", \"diagnosis\": \"diabetes\", \"weight\": 210, \"smoking\": \"yes\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VGrNexUd8dUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T381wqHf8dSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsletter = \"\"\"\n",
        "European AI champion Mistral AI unveiled new large language models and formed an alliance with Microsoft.\n",
        "\n",
        "What’s new: Mistral AI introduced two closed models, Mistral Large and Mistral Small (joining Mistral Medium, which debuted quietly late last year). Microsoft invested $16.3 million in the French startup, and it agreed to distribute Mistral Large on its Azure platform and let Mistral AI use Azure computing infrastructure. Mistral AI makes the new models available to try for free here and to use on its La Plateforme and via custom deployments.\n",
        "\n",
        "Model specs: The new models’ parameter counts, architectures, and training methods are undisclosed. Like the earlier, open source Mistral 7B and Mixtral 8x7B, they can process 32,000 tokens of input context.\n",
        "\n",
        "Mistral Large achieved 81.2 percent on the MMLU benchmark, outperforming Anthropic’s Claude 2, Google’s Gemini Pro, and Meta’s Llama 2 70B, though falling short of GPT-4. Mistral Small, which is optimized for latency and cost, achieved 72.2 percent on MMLU.\n",
        "Both models are fluent in French, German, Spanish, and Italian. They’re trained for function calling and JSON-format output.\n",
        "Microsoft’s investment in Mistral AI is significant but tiny compared to its $13 billion stake in OpenAI and Google and Amazon’s investments in Anthropic, which amount to $2 billion and $4 billion respectively.\n",
        "Mistral AI and Microsoft will collaborate to train bespoke models for customers including European governments.\n",
        "Behind the news: Mistral AI was founded in early 2023 by engineers from Google and Meta. The French government has touted the company as a home-grown competitor to U.S.-based leaders like OpenAI. France’s representatives in the European Commission argued on Mistral’s behalf to loosen the European Union’s AI Act oversight on powerful AI models.\n",
        "\n",
        "Yes, but: Mistral AI’s partnership with Microsoft has divided European lawmakers and regulators. The European Commission, which already was investigating Microsoft’s agreement with OpenAI for potential breaches of antitrust law, plans to investigate the new partnership as well. Members of President Emmanuel Macron’s Renaissance party criticized the deal’s potential to give a U.S. company access to European users’ data. However, other French lawmakers support the relationship.\n",
        "\n",
        "Why it matters: The partnership between Mistral AI and Microsoft gives the startup crucial processing power for training large models and greater access to potential customers around the world. It gives the tech giant greater access to the European market. And it gives Azure customers access to a high-performance model that’s tailored to Europe’s unique regulatory environment.\n",
        "\n",
        "We’re thinking: Mistral AI has made impressive progress in a short time, especially relative to the resources at its disposal as a startup. Its partnership with a leading hyperscaler is a sign of the tremendous processing and distribution power that remains concentrated in the large, U.S.-headquartered cloud companies.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7_bY8LWW7Nv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are a commentator. Your task is to write a report on a newsletter.\n",
        "When presented with the newsletter, come up with interesting questions to ask,\n",
        "and answer each question.\n",
        "Afterward, combine all the information and write a report in the markdown\n",
        "format.\n",
        "\n",
        "# Newsletter:\n",
        "{newsletter}\n",
        "\n",
        "# Instructions:\n",
        "## Summarize:\n",
        "In clear and concise language, summarize the key points and themes\n",
        "presented in the newsletter.\n",
        "\n",
        "## Interesting Questions:\n",
        "Generate three distinct and thought-provoking questions that can be\n",
        "asked about the content of the newsletter. For each question:\n",
        "- After \"Q: \", describe the problem\n",
        "- After \"A: \", provide a detailed explanation of the problem addressed\n",
        "in the question.\n",
        "- Enclose the ultimate answer in <>.\n",
        "\n",
        "## Write a analysis report\n",
        "Using the summary and the answers to the interesting questions,\n",
        "create a comprehensive report in Markdown format.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bWaA2NSx7PIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = mistral(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRkOaUhi92_P",
        "outputId": "90c97d75-3fe6-496c-f2f0-c3e43b9cbbf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Summary:\n",
            "European AI startup Mistral AI has introduced two new large language models, Mistral Large and Mistral Small, and formed a strategic alliance with Microsoft. Microsoft has invested $16.3 million in Mistral AI, allowing the startup to use Azure's computing infrastructure and distribute Mistral Large on Azure. The partnership has stirred controversy among European lawmakers due to potential data access issues and antitrust concerns.\n",
            "\n",
            "# Interesting Questions:\n",
            "\n",
            "Q: How do Mistral AI's new models compare to competitors in terms of performance?\n",
            "A: Mistral Large outperformed several competitors on the MMLU benchmark, including Anthropic's Claude 2, Google's Gemini Pro, and Meta's Llama 2 70B, though it did not surpass GPT-4. Mistral Small, optimized for latency and cost, also performed well, scoring 72.2% on MMLU.\n",
            "\n",
            "<Mistral Large outperforms several competitors but not GPT-4, and Mistral Small also performs well relative to its optimization goals.>\n",
            "\n",
            "Q: What are the potential implications of Mistral AI's partnership with Microsoft for the European AI market?\n",
            "A: The partnership could provide Mistral AI with the processing power needed to train large models and broader customer access, while giving Microsoft greater access to the European market. Azure customers would also gain access to a high-performance model tailored to Europe's regulatory environment.\n",
            "\n",
            "<The partnership could lead to increased competition in the European AI market and provide new capabilities to Azure customers.>\n",
            "\n",
            "Q: How has Mistral AI's partnership with Microsoft been received by European lawmakers and regulators?\n",
            "A: The partnership has been met with mixed reactions. While some French lawmakers support the relationship, others have criticized it for potentially giving a U.S. company access to European users' data. The European Commission is planning to investigate the partnership for potential antitrust breaches.\n",
            "\n",
            "<The partnership has divided European lawmakers and regulators due to data access and antitrust concerns.>\n",
            "\n",
            "# Analysis Report:\n",
            "\n",
            "## Mistral AI's New Models and Performance\n",
            "Mistral AI has introduced two new large language models, Mistral Large and Mistral Small. While specific details about the models' architecture and training methods are undisclosed, their performance on the MMLU benchmark demonstrates their competitiveness. Mistral Large outperformed several industry models, highlighting Mistral AI's progress despite its startup status.\n",
            "\n",
            "## Implications of the Mistral AI-Microsoft Partnership\n",
            "The partnership between Mistral AI and Microsoft could significantly impact the European AI market. Mistral AI gains crucial processing power and broader customer reach, while Microsoft expands its presence in Europe. Azure customers also stand to benefit from access to a high-performance model attuned to European regulatory norms.\n",
            "\n",
            "## Regulatory and Political Reactions to the Partnership\n",
            "The partnership has sparked debate among European lawmakers and regulators. Concerns over data access and potential antitrust breaches have led to criticism and plans for investigation by the European Commission. However, the partnership also has supporters who see it as a means to bolster European AI competitiveness.\n",
            "\n",
            "In conclusion, Mistral AI's new models and partnership with Microsoft represent a significant development in the European AI landscape. However, the partnership's implications for data privacy and market competition will continue to be a subject of scrutiny and debate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Classify the following email to determine if it is spam or not.\n",
        "Only respond with the exact text \"Spam\" or \"Not Spam\".\n",
        "\n",
        "# Email:\n",
        "🎉 Urgent! You've Won a $1,000,000 Cash Prize!\n",
        "💰 To claim your prize, please click on the link below:\n",
        "https://bit.ly/claim-your-prize\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_luNUKqX94T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mistral(prompt, model=\"mistral-small-latest\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G4ltJDpS-AdJ",
        "outputId": "c70cb68b-d31c-4cc6-b6f9-aeca670e4a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Spam'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Compose a welcome email for new customers who have just made\n",
        "their first purchase with your product.\n",
        "Start by expressing your gratitude for their business,\n",
        "and then convey your excitement for having them as a customer.\n",
        "Include relevant details about their recent order.\n",
        "Sign the email with \"The Fun Shop Team\".\n",
        "\n",
        "Order details:\n",
        "- Customer name: Anna\n",
        "- Product: hat\n",
        "- Estimate date of delivery: Feb. 25, 2024\n",
        "- Return policy: 30 days\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "asjLyw7r-Ap1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response_medium = mistral(prompt, model=\"mistral-medium-latest\")"
      ],
      "metadata": {
        "id": "I9MptEw_-Cs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response_medium)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-nCayEk-DzW",
        "outputId": "71c47d93-b1d8-470e-bc45-3d0406132c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear Anna,\n",
            "\n",
            "We are thrilled to welcome you to The Fun Shop family! We are so grateful for your business and are excited to have you as a customer.\n",
            "\n",
            "We wanted to take a moment to confirm your recent order with us. You have purchased a beautiful hat that is sure to add style and fun to your wardrobe. Your estimated delivery date is February 25, 2024, and we will make sure to keep you updated on the status of your order every step of the way.\n",
            "\n",
            "At The Fun Shop, we stand behind the quality of our products and are committed to providing excellent customer service. That's why we offer a 30-day return policy on all purchases. If for any reason you are not completely satisfied with your hat, simply let us know and we will make it right.\n",
            "\n",
            "Thank you again for choosing The Fun Shop for your shopping needs. We look forward to serving you in the future and hope you enjoy your new hat!\n",
            "\n",
            "Best regards,\n",
            "The Fun Shop Team\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Calculate the difference in payment dates between the two \\\n",
        "customers whose payment amounts are closest to each other \\\n",
        "in the following dataset. Do not write code.\n",
        "\n",
        "# dataset:\n",
        "'{\n",
        "  \"transaction_id\":{\"0\":\"T1001\",\"1\":\"T1002\",\"2\":\"T1003\",\"3\":\"T1004\",\"4\":\"T1005\"},\n",
        "    \"customer_id\":{\"0\":\"C001\",\"1\":\"C002\",\"2\":\"C003\",\"3\":\"C002\",\"4\":\"C001\"},\n",
        "    \"payment_amount\":{\"0\":125.5,\"1\":89.99,\"2\":120.0,\"3\":54.3,\"4\":210.2},\n",
        "\"payment_date\":{\"0\":\"2021-10-05\",\"1\":\"2021-10-06\",\"2\":\"2021-10-07\",\"3\":\"2021-10-05\",\"4\":\"2021-10-08\"},\n",
        "    \"payment_status\":{\"0\":\"Paid\",\"1\":\"Unpaid\",\"2\":\"Paid\",\"3\":\"Paid\",\"4\":\"Pending\"}\n",
        "}'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eusKpWA_-E5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_small = mistral(prompt, model=\"mistral-small-latest\")"
      ],
      "metadata": {
        "id": "oRh-j3Tc-GH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response_small)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cdgkh6r-Hiq",
        "outputId": "d21f93f2-6b3f-43af-a321-d798f1ac1995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To find the difference in payment dates between the two customers whose payment amounts are closest to each other, first we need to find the two closest payment amounts in the dataset.\n",
            "\n",
            "The two closest payment amounts in the dataset are 120.0 and 125.5, which belong to customer_id C003 and C001, respectively.\n",
            "\n",
            "Now, let's find the payment dates for these customers.\n",
            "- C001's payment date is 2021-10-08.\n",
            "- C003's payment date is 2021-10-07.\n",
            "\n",
            "The difference in payment dates between the two customers is:\n",
            "2021-10-08 (C001's payment date) - 2021-10-07 (C003's payment date) = 1 day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_large = mistral(prompt, model=\"mistral-large-latest\")"
      ],
      "metadata": {
        "id": "mjUslRjE-IyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response_large)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC8RuLDp-KLG",
        "outputId": "99bf63a5-54f9-45d9-846b-da1a9b5f44ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First, we need to identify the two customers with the closest payment amounts. By examining the dataset, we can see that the closest payment amounts are $125.50 (T1001) and $120.00 (T1003), with a difference of $5.50.\n",
            "\n",
            "Next, we need to calculate the difference in payment dates between these two transactions. The payment date for T1001 is 2021-10-05, and the payment date for T1003 is 2021-10-07.\n",
            "\n",
            "To calculate the difference between two dates, we subtract the earlier date from the later date. In this case, the difference is 2 days, as 2021-10-07 (T1003) is 2 days later than 2021-10-05 (T1001).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = \"\"\"\n",
        "McDonald's: 8.40\n",
        "Safeway: 10.30\n",
        "Carrefour: 15.00\n",
        "Toys R Us: 20.50\n",
        "Panda Express: 10.20\n",
        "Beanie Baby Outlet: 25.60\n",
        "World Food Wraps: 22.70\n",
        "Stuffed Animals Shop: 45.10\n",
        "Sanrio Store: 85.70\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Given the purchase details, how much did I spend on each category:\n",
        "1) restaurants\n",
        "2) groceries\n",
        "3) stuffed animals and props\n",
        "{transactions}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "S2uh7xch-Lek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_small = mistral(prompt, model=\"mistral-small-latest\")\n",
        "print(response_small)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-yRE3Uo-Myq",
        "outputId": "17ab0d8e-db8a-46aa-82ea-5a8fff56ff71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's the breakdown of your spending by category:\n",
            "\n",
            "1) restaurants:\n",
            "   - McDonald's: 8.40\n",
            "   - Panda Express: 10.20\n",
            "     Total: 18.60\n",
            "\n",
            "2) groceries:\n",
            "   - Safeway: 10.30\n",
            "   - Carrefour: 15.00\n",
            "   - World Food Wraps: 22.70\n",
            "     Total: 48.00\n",
            "\n",
            "3) stuffed animals and props:\n",
            "   - Toys R Us: 20.50\n",
            "   - Beanie Baby Outlet: 25.60\n",
            "   - Stuffed Animals Shop: 45.10\n",
            "   - Sanrio Store: 85.70\n",
            "     Total: 176.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_large = mistral(prompt, model=\"mistral-large-latest\")\n",
        "print(response_large)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZk-k2lQ-OAd",
        "outputId": "d90912f3-31b4-438a-ce9a-f9344b8460f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, let's categorize your expenses:\n",
            "\n",
            "1) Restaurants:\n",
            "   - McDonald's: $8.40\n",
            "   - Panda Express: $10.20\n",
            "   - World Food Wraps: $22.70\n",
            "   Total spent on restaurants: $8.40 + $10.20 + $22.70 = $41.30\n",
            "\n",
            "2) Groceries:\n",
            "   - Safeway: $10.30\n",
            "   - Carrefour: $15.00\n",
            "   Total spent on groceries: $10.30 + $15.00 = $25.30\n",
            "\n",
            "3) Stuffed animals and props:\n",
            "   - Toys R Us: $20.50\n",
            "   - Beanie Baby Outlet: $25.60\n",
            "   - Stuffed Animals Shop: $45.10\n",
            "   - Sanrio Store: $85.70\n",
            "   Total spent on stuffed animals and props: $20.50 + $25.60 + $45.10 + $85.70 = $176.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = \"\"\"\n",
        "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
        "\n",
        "You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
        "\n",
        "You can return the answer in any order.\n",
        "\n",
        "Your code should pass these tests:\n",
        "\n",
        "assert twoSum([2,7,11,15], 9) == [0,1]\n",
        "assert twoSum([3,2,4], 6) == [1,2]\n",
        "assert twoSum([3,3], 6) == [0,1]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MTb9QlN4-PLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(mistral(user_message, model=\"mistral-large-latest\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9mBHNu6-Qjp",
        "outputId": "cf5552fc-17e9-4490-adbe-097c34c5ecdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here is a Python solution for your problem. This solution uses a dictionary to store the numbers in the array as it iterates through them. For each number, it checks if the target minus the current number is in the dictionary. If it is, it means we have found two numbers that add up to the target, so we return their indices. If not, we add the current number and its index to the dictionary and continue to the next number.\n",
            "\n",
            "```python\n",
            "def twoSum(nums, target):\n",
            "    num_dict = {}\n",
            "    for i, num in enumerate(nums):\n",
            "        if target - num in num_dict:\n",
            "            return [num_dict[target - num], i]\n",
            "        num_dict[num] = i\n",
            "```\n",
            "\n",
            "You can then call this function with your test cases like this:\n",
            "\n",
            "```python\n",
            "assert twoSum([2,7,11,15], 9) == [0,1]\n",
            "assert twoSum([3,2,4], 6) == [1,2]\n",
            "assert twoSum([3,3], 6) == [0,1]\n",
            "```\n",
            "\n",
            "This function should pass all your tests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = \"\"\"\n",
        "Lequel est le plus lourd une livre de fer ou un kilogramme de plume\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FD4c1av4-Ryf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mistral(user_message, model=\"mistral-large-latest\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM7VeV1v-UYG",
        "outputId": "1ba9626e-f592-4cfe-da79-64321c111369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Une livre de fer pèse environ 0,45 kilogramme, tandis qu'un kilogramme de plumes pèse un kilogramme. Par conséquent, un kilogramme de plumes est plus lourd qu'une livre de fer.\n",
            "\n",
            "Cependant, il est important de noter que cette comparaison est un peu trompeuse car une livre et un kilogramme sont des unités de mesure différentes. Si l'on comparait une livre de plumes à une livre de fer, la masse serait la même, mais le volume occupé par les plumes serait beaucoup plus grand que celui occupé par le fer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Classify the following email to determine if it is spam or not.\n",
        "Only respond with the exact text \"Spam\" or \"Not Spam\".\n",
        "\n",
        "# Email:\n",
        "🎉 Urgent! You've Won a $1,000,000 Cash Prize!\n",
        "💰 To claim your prize, please click on the link below:\n",
        "https://bit.ly/claim-your-prize\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "E-D7jCEY-Vjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mistral(prompt, model=\"mistral-small-latest\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jn_Kopbv-a5w",
        "outputId": "850e1397-8d29-46b2-d484-6fb3ecad1c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Spam'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Compose a welcome email for new customers who have just made\n",
        "their first purchase with your product.\n",
        "Start by expressing your gratitude for their business,\n",
        "and then convey your excitement for having them as a customer.\n",
        "Include relevant details about their recent order.\n",
        "Sign the email with \"The Fun Shop Team\".\n",
        "\n",
        "Order details:\n",
        "- Customer name: Anna\n",
        "- Product: hat\n",
        "- Estimate date of delivery: Feb. 25, 2024\n",
        "- Return policy: 30 days\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nYKRPDCL-cVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_medium = mistral(prompt, model=\"mistral-medium-latest\")\n",
        "print(response_medium)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InovrVjg-dmr",
        "outputId": "5497db48-4140-4aae-84c8-f7b6c4a1c834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: Welcome to The Fun Shop, Anna! Thank you for your first purchase.\n",
            "\n",
            "Dear Anna,\n",
            "\n",
            "We are thrilled to have you as a new customer at The Fun Shop! We appreciate your decision to choose our store for your shopping needs. We hope that your experience with us has been enjoyable so far.\n",
            "\n",
            "We are excited to let you know that your recent order for a hat has been processed successfully. Your estimated delivery date is February 25, 2024, and we will make sure that your order arrives at your doorstep on time.\n",
            "\n",
            "In case you have any questions about your order, please do not hesitate to contact our customer service team at [support@thefunshop.com](mailto:support@thefunshop.com). We are always here to help!\n",
            "\n",
            "We would also like to take this opportunity to remind you of our 30-day return policy. If for any reason you are not completely satisfied with your purchase, you can return it within 30 days of delivery for a full refund or exchange.\n",
            "\n",
            "Once again, thank you for choosing The Fun Shop for your shopping needs. We look forward to serving you again in the future.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "The Fun Shop Team\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Calculate the difference in payment dates between the two \\\n",
        "customers whose payment amounts are closest to each other \\\n",
        "in the following dataset. Do not write code.\n",
        "\n",
        "# dataset:\n",
        "'{\n",
        "  \"transaction_id\":{\"0\":\"T1001\",\"1\":\"T1002\",\"2\":\"T1003\",\"3\":\"T1004\",\"4\":\"T1005\"},\n",
        "    \"customer_id\":{\"0\":\"C001\",\"1\":\"C002\",\"2\":\"C003\",\"3\":\"C002\",\"4\":\"C001\"},\n",
        "    \"payment_amount\":{\"0\":125.5,\"1\":89.99,\"2\":120.0,\"3\":54.3,\"4\":210.2},\n",
        "\"payment_date\":{\"0\":\"2021-10-05\",\"1\":\"2021-10-06\",\"2\":\"2021-10-07\",\"3\":\"2021-10-05\",\"4\":\"2021-10-08\"},\n",
        "    \"payment_status\":{\"0\":\"Paid\",\"1\":\"Unpaid\",\"2\":\"Paid\",\"3\":\"Paid\",\"4\":\"Pending\"}\n",
        "}'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NsAVJYNV-e-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "nqHpavxP-uGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"transaction_id\": [\"T1001\", \"T1002\", \"T1003\", \"T1004\", \"T1005\"],\n",
        "    \"customer_id\": [\"C001\", \"C002\", \"C003\", \"C002\", \"C001\"],\n",
        "    \"payment_amount\": [125.50, 89.99, 120.00, 54.30, 210.20],\n",
        "    \"payment_date\": [\n",
        "        \"2021-10-05\",\n",
        "        \"2021-10-06\",\n",
        "        \"2021-10-07\",\n",
        "        \"2021-10-05\",\n",
        "        \"2021-10-08\",\n",
        "    ],\n",
        "    \"payment_status\": [\"Paid\", \"Unpaid\", \"Paid\", \"Paid\", \"Pending\"],\n",
        "}\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "ZmINmlKJ-gRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BIVJugxr-rxE",
        "outputId": "406bd248-8191-41d7-9e31-50f1acf3784b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  transaction_id customer_id  payment_amount payment_date payment_status\n",
              "0          T1001        C001          125.50   2021-10-05           Paid\n",
              "1          T1002        C002           89.99   2021-10-06         Unpaid\n",
              "2          T1003        C003          120.00   2021-10-07           Paid\n",
              "3          T1004        C002           54.30   2021-10-05           Paid\n",
              "4          T1005        C001          210.20   2021-10-08        Pending"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4e61d584-b265-4078-a936-4167d20c195f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>transaction_id</th>\n",
              "      <th>customer_id</th>\n",
              "      <th>payment_amount</th>\n",
              "      <th>payment_date</th>\n",
              "      <th>payment_status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>T1001</td>\n",
              "      <td>C001</td>\n",
              "      <td>125.50</td>\n",
              "      <td>2021-10-05</td>\n",
              "      <td>Paid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>T1002</td>\n",
              "      <td>C002</td>\n",
              "      <td>89.99</td>\n",
              "      <td>2021-10-06</td>\n",
              "      <td>Unpaid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>T1003</td>\n",
              "      <td>C003</td>\n",
              "      <td>120.00</td>\n",
              "      <td>2021-10-07</td>\n",
              "      <td>Paid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>T1004</td>\n",
              "      <td>C002</td>\n",
              "      <td>54.30</td>\n",
              "      <td>2021-10-05</td>\n",
              "      <td>Paid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>T1005</td>\n",
              "      <td>C001</td>\n",
              "      <td>210.20</td>\n",
              "      <td>2021-10-08</td>\n",
              "      <td>Pending</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e61d584-b265-4078-a936-4167d20c195f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4e61d584-b265-4078-a936-4167d20c195f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4e61d584-b265-4078-a936-4167d20c195f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-012c4204-1bac-4ce8-a3bc-d8b0750aaaa1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-012c4204-1bac-4ce8-a3bc-d8b0750aaaa1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-012c4204-1bac-4ce8-a3bc-d8b0750aaaa1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c8aadae3-f04d-4100-9e27-80b287334b6c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c8aadae3-f04d-4100-9e27-80b287334b6c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"transaction_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"T1002\",\n          \"T1005\",\n          \"T1003\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"customer_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"C001\",\n          \"C002\",\n          \"C003\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"payment_amount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57.8432798171058,\n        \"min\": 54.3,\n        \"max\": 210.2,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          89.99,\n          210.2,\n          120.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"payment_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"2021-10-06\",\n          \"2021-10-08\",\n          \"2021-10-05\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"payment_status\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Paid\",\n          \"Unpaid\",\n          \"Pending\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"\n",
        "    \"transaction_id\": [\"T1001\", \"T1002\", \"T1003\", \"T1004\", \"T1005\"],\n",
        "    \"customer_id\": [\"C001\", \"C002\", \"C003\", \"C002\", \"C001\"],\n",
        "    \"payment_amount\": [125.50, 89.99, 120.00, 54.30, 210.20],\n",
        "    \"payment_date\": [\n",
        "        \"2021-10-05\",\n",
        "        \"2021-10-06\",\n",
        "        \"2021-10-07\",\n",
        "        \"2021-10-05\",\n",
        "        \"2021-10-08\",\n",
        "    ],\n",
        "    \"payment_status\": [\"Paid\", \"Unpaid\", \"Paid\", \"Paid\", \"Pending\"],\n",
        "}\n",
        "\"\"\"\n",
        "transaction_id = \"T1001\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Given the following data, what is the payment status for \\\n",
        " transaction_id={transaction_id}?\n",
        "\n",
        "data:\n",
        "{data}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GU-h8Mxb-vXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = mistral(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J5rXLO_-xnH",
        "outputId": "f98995ea-bdfb-47df-878d-8700b74f2f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The payment status for transaction_id=T1001 is \"Paid\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def retrieve_payment_status(df: data, transaction_id: str) -> str:\n",
        "    if transaction_id in df.transaction_id.values:\n",
        "        return json.dumps(\n",
        "            {\"status\": df[df.transaction_id == transaction_id].payment_status.item()}\n",
        "        )\n",
        "    return json.dumps({\"error\": \"transaction id not found.\"})\n",
        "status = retrieve_payment_status(df, transaction_id=\"T1001\")\n",
        "print(status)\n",
        "type(status)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CQTnV7_-zOF",
        "outputId": "db5f708c-1e29-4946-89d5-b4dbec140ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"status\": \"Paid\"}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_payment_date(df: data, transaction_id: str) -> str:\n",
        "    if transaction_id in df.transaction_id.values:\n",
        "        return json.dumps(\n",
        "            {\"date\": df[df.transaction_id == transaction_id].payment_date.item()}\n",
        "        )\n",
        "    return json.dumps({\"error\": \"transaction id not found.\"})"
      ],
      "metadata": {
        "id": "015FEFLe-4CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date = retrieve_payment_date(df, transaction_id=\"T1002\")\n",
        "print(date)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ5zMawo-6G9",
        "outputId": "634a1d82-5234-4823-d88e-f2d92329d738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"date\": \"2021-10-06\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tool_payment_status = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"retrieve_payment_status\",\n",
        "        \"description\": \"Get payment status of a transaction\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"transaction_id\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The transaction id.\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"transaction_id\"],\n",
        "        },\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "yOMowHe4-7O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(tool_payment_status)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXkdtf2Y_AWd",
        "outputId": "2a67e380-bce1-454b-ba16-73306b0a60bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tool_payment_date = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"retrieve_payment_date\",\n",
        "        \"description\": \"Get payment date of a transaction\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"transaction_id\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The transaction id.\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"transaction_id\"],\n",
        "        },\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "PdmK7Hjg-8YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(tool_payment_status)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQkL3glE--By",
        "outputId": "b6713ca6-e6e5-43ab-b182-554d193d29ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [tool_payment_status, tool_payment_date]"
      ],
      "metadata": {
        "id": "gxF-tRZu_Cm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkQdclbS_D2r",
        "outputId": "6669a4c9-c886-4cc5-b542-d013e69f5b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCld4wn2_E4W",
        "outputId": "a55f15c8-45d7-42fe-8048-8d2758f1288c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'type': 'function',\n",
              "  'function': {'name': 'retrieve_payment_status',\n",
              "   'description': 'Get payment status of a transaction',\n",
              "   'parameters': {'type': 'object',\n",
              "    'properties': {'transaction_id': {'type': 'string',\n",
              "      'description': 'The transaction id.'}},\n",
              "    'required': ['transaction_id']}}},\n",
              " {'type': 'function',\n",
              "  'function': {'name': 'retrieve_payment_date',\n",
              "   'description': 'Get payment date of a transaction',\n",
              "   'parameters': {'type': 'object',\n",
              "    'properties': {'transaction_id': {'type': 'string',\n",
              "      'description': 'The transaction id.'}},\n",
              "    'required': ['transaction_id']}}}]"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "names_to_functions = {\n",
        "    \"retrieve_payment_status\": functools.partial(retrieve_payment_status, df=df),\n",
        "    \"retrieve_payment_date\": functools.partial(retrieve_payment_date, df=df),\n",
        "}"
      ],
      "metadata": {
        "id": "zPVY-ibM_F67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names_to_functions[\"retrieve_payment_status\"](transaction_id=\"T1001\")\n",
        "tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEl-bm9V_HR2",
        "outputId": "b60d94fe-dcee-4012-a410-8397b0718d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'type': 'function',\n",
              "  'function': {'name': 'retrieve_payment_status',\n",
              "   'description': 'Get payment status of a transaction',\n",
              "   'parameters': {'type': 'object',\n",
              "    'properties': {'transaction_id': {'type': 'string',\n",
              "      'description': 'The transaction id.'}},\n",
              "    'required': ['transaction_id']}}},\n",
              " {'type': 'function',\n",
              "  'function': {'name': 'retrieve_payment_date',\n",
              "   'description': 'Get payment date of a transaction',\n",
              "   'parameters': {'type': 'object',\n",
              "    'properties': {'transaction_id': {'type': 'string',\n",
              "      'description': 'The transaction id.'}},\n",
              "    'required': ['transaction_id']}}}]"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mistralai.models.chat_completion import ChatMessage\n",
        "\n",
        "chat_history = [\n",
        "    ChatMessage(role=\"user\", content=\"What's the status of my transaction?\")\n",
        "]"
      ],
      "metadata": {
        "id": "0kcOr7E__IUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mua9exB_K9s",
        "outputId": "a438bde5-c4bf-419e-fcb6-4a6df04cba8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ChatMessage(role='user', content=\"What's the status of my transaction?\", name=None, tool_calls=None, tool_call_id=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mistralai.models.chat_completion import ChatMessage\n",
        "\n",
        "chat_history = [\n",
        "    ChatMessage(role=\"user\", content=\"What's the status of my transaction?\")\n",
        "]"
      ],
      "metadata": {
        "id": "OS6ZjAKRAayN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mistralai.client import MistralClient\n",
        "\n",
        "# Set the model to use\n",
        "model = \"mistral-large-latest\"\n",
        "\n",
        "\n",
        "\n",
        "# Set the model to use\n",
        "model = \"mistral-large-latest\"\n",
        "\n",
        "# Initialize the client\n",
        "client = MistralClient()\n",
        "# Define the chat history\n",
        "chat_history = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
        "]\n",
        "\n",
        "# Define any tools to use (none in this case)\n",
        "tools = []\n",
        "\n",
        "# Send a chat request to the model\n",
        "response = client.chat(model=model, messages=chat_history, tools=tools, tool_choice=\"auto\")\n",
        "\n",
        "# Print the response\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPwMBV0y_z3k",
        "outputId": "1fadd109-a7df-4833-8b99-ea93a259a987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id='a16f4c276b254556acf46ac759e78176' object='chat.completion' created=1721142418 model='mistral-large-latest' choices=[ChatCompletionResponseChoice(index=0, message=ChatMessage(role='assistant', content=\"I'm an AI and don't have feelings, but I'm here and ready to help you. How can I assist you today?\", name=None, tool_calls=None, tool_call_id=None), finish_reason=<FinishReason.stop: 'stop'>)] usage=UsageInfo(prompt_tokens=17, total_tokens=47, completion_tokens=30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gj1BDZWM_3G9",
        "outputId": "4a0b892b-246f-4fff-c286-d48068a4164e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm an AI and don't have feelings, but I'm here and ready to help you. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.append(\n",
        "    ChatMessage(role=\"assistant\", content=response.choices[0].message.content)\n",
        ")\n",
        "chat_history.append(ChatMessage(role=\"user\", content=\"My transaction ID is T1001.\"))\n",
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlijEdUFAW6i",
        "outputId": "244bea6a-4323-4c97-f1a0-2aaa61e7e8d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
              " {'role': 'user', 'content': 'Hello, how are you?'},\n",
              " ChatMessage(role='assistant', content=\"I'm an AI and don't have feelings, but I'm here and ready to help you. How can I assist you today?\", name=None, tool_calls=None, tool_call_id=None),\n",
              " ChatMessage(role='user', content='My transaction ID is T1001.', name=None, tool_calls=None, tool_call_id=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat(\n",
        "    model=model, messages=chat_history, tools=tools, tool_choice=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "lkS1yokNAe2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehshPriwAhRj",
        "outputId": "6c68695d-675b-4e6d-895c-7bd0e5996592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionResponse(id='9e2070d2b11a4f6aa9c956ee6e163d6a', object='chat.completion', created=1721142419, model='mistral-large-latest', choices=[ChatCompletionResponseChoice(index=0, message=ChatMessage(role='assistant', content='Thank you for providing your transaction ID. How may I assist you with this transaction? Do you have a specific question or issue?', name=None, tool_calls=None, tool_call_id=None), finish_reason=<FinishReason.stop: 'stop'>)], usage=UsageInfo(prompt_tokens=60, total_tokens=86, completion_tokens=26))"
            ]
          },
          "metadata": {},
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.append(response.choices[0].message)"
      ],
      "metadata": {
        "id": "ivDPRNPbAj2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need to work on functions\n"
      ],
      "metadata": {
        "id": "AdUEdnesAonB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "response = requests.get(\n",
        "    \"https://www.deeplearning.ai/the-batch/a-roadmap-explores-how-ai-can-detect-and-mitigate-greenhouse-gases/\"\n",
        ")\n",
        "html_doc = response.text\n",
        "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
        "tag = soup.find(\"div\", re.compile(\"^prose--styled\"))\n",
        "text = tag.text\n",
        "print(text)"
      ],
      "metadata": {
        "id": "hNZSQiZqArSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce3c34d-43f5-4494-803f-d4850f02fc8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How can AI help to fight climate change? A new report evaluates progress so far and explores options for the future.What’s new: The Innovation for Cool Earth Forum, a conference of climate researchers hosted by Japan, published a roadmap for the use of data science, computer vision, and AI-driven simulation to reduce greenhouse gas emissions. The roadmap evaluates existing approaches and suggests ways to scale them up.How it works: The roadmap identifies 6 “high-potential opportunities”: activities in which AI systems can make a significant difference based on the size of the opportunity, real-world results, and validated research. The authors emphasize the need for data, technical and scientific talent, computing power, funding, and leadership to take advantage of these opportunities.Monitoring emissions. AI systems analyze data from satellites, drones, and ground sensors to measure greenhouse gas emissions. The European Union uses them to measure methane emissions, environmental organizations gauge carbon monoxide emissions to help guide the carbon offset trading market, and consultancies like Kayrros identify large-scale sources of greenhouse gasses like landfills and oilfields. The authors recommend an impartial clearinghouse for climate-related data and wider access to satellite data.Energy. More than 30 percent of carbon emissions come from generating electricity. Simulations based on neural networks are helping to predict power generated by wind and solar plants and demand on electrical grids, which have proven to be difficult for other sorts of algorithms. AI systems also help to situate wind and solar plants and optimize grids. These approaches could scale up with more robust models, standards to evaluate performance, and security protocols.Manufacturing. An unnamed Brazilian steelmaker has used AI to measure the chemical composition of scrap metal to be reused batch by batch, allowing it to reduce carbon-intensive additives by 8 percent while improving overall quality. AI systems can analyze historical data to help factories use more recycled materials, cut waste, minimize energy use, and reduce downtime. Similarly, they can optimize supply chains to reduce emissions contributed by logistics. Agriculture. Farmers use AI-equipped sensors to simulate different crop rotations and weather events to forecast crop yield or loss. Armed with this data, food producers can cut waste and reduce carbon footprints. The authors cite lack of food-related datasets and investment in adapting farming practices as primary barriers to taking full advantage of AI in the food industry.Transportation. AI systems can reduce greenhouse-gas emissions by improving traffic flow, ameliorating congestion, and optimizing public transportation. Moreover, reinforcement learning can reduce the impact of electric vehicles on the power grid by optimizing their charging. More data, uniform standards, and AI talent are needed to realize this potential.Materials. Materials scientists use AI models to study traits of existing materials and design new ones. These techniques could accelerate development of more efficient batteries, solar cells, wind turbines, and transmission infrastructure. Better coordination between materials scientists and AI researchers would accelerate such benefits.Why it matters: AI has demonstrated its value in identifying sources of emissions, optimizing energy consumption, and developing and understanding materials. Scaling and extending this value in areas that generate the most greenhouse gasses — particularly energy generation, manufacturing, food production, and transportation — could make a significant dent in greenhouse gas emissions.We’re thinking: AI also has an important role to play in advancing the science of climate geoengineering, such as stratospheric aerosol injection (SAI), to cool down the planet. More research is needed to determine whether SAI is a good idea, but AI-enabled climate modeling will help answer this question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"AI_Article.txt\"\n",
        "with open(file_name, 'w') as file:\n",
        "    file.write(text)"
      ],
      "metadata": {
        "id": "myMccqpIBs2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 512\n",
        "chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]"
      ],
      "metadata": {
        "id": "li6nSpQUCPVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKGvjDNoC_Rg",
        "outputId": "c70348ad-e2fa-4dc5-96ee-16915c96c7db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 264
        }
      ]
    }
  ]
}